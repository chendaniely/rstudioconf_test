---
title: "Regression"
subtitle: "K-NN and Linear Regression"
format:
  revealjs:
    slide-number: true
    slide-level: 4
    smaller: true
    theme: simple
jupyter: python3

execute:
  echo: true
  warning: false

editor:
  render-on-save: true
---

```{python}
#| include: false
import pandas as pd
pd.set_option('display.max_rows', 5)
```

## Session learning objectives: KNN

- Recognize situations where a regression analysis would be appropriate for making predictions.
- Explain the K-nearest neighbors (K-NN) regression algorithm and describe how it differs from K-NN classification.
- Interpret the output of a K-NN regression.
- In a data set with two or more variables, perform K-nearest neighbors regression in Python.
- Evaluate K-NN regression prediction quality in Python using the root mean squared prediction error (RMSPE).
- Estimate the RMSPE in Python using cross-validation or a test set.
- Choose the number of neighbors in K-nearest neighbors regression by minimizing estimated cross-validation RMSPE.
- Describe underfitting and overfitting, and relate it to the number of neighbors in K-nearest neighbors regression.
- Describe the advantages and disadvantages of K-nearest neighbors regression.


## Session learning objective: Linear Regression

- Use Python to fit simple and multivariable linear regression models on training data.
- Evaluate the linear regression model on test data.
- Compare and contrast predictions obtained from K-nearest neighbors regression to those obtained using linear regression from the same data set.
- Describe how linear regression is affected by outliers and multicollinearity.

## The regression problem

- Predictive problem
- Use past information to predict future observations
- Predict *numerical* values instead of *categorical* values

Examples:

- Race time in the Boston marathon
- size of a house to predict its sale price

### Regression Methods

In this workshop:

- K-nearest neighbors
- Linear regression

### Classification similarities to regression

Concepts from classification map over to the setting of regression

- Predict a new observation's response variable based on past observations
- Split the data into training and test sets
- Use cross-validation to evaluate different choices of model parameters

### Difference

Predicting numerical variables instead of categorical variables

## Explore a data set

[932 real estate transactions in Sacramento, California](https://support.spatialkey.com/spatialkey-sample-csv-data/)

> Can we use the size of a house in the Sacramento, CA area to predict its sale price?

### Data and package setup

```{python}
import altair as alt
import numpy as np
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.compose import make_column_transformer
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn import set_config

# Output dataframes instead of arrays
set_config(transform_output='pandas')

sacramento = pd.read_csv('data/sacramento.csv')
print(sacramento)

```

### Price vs Sq.Ft

```{python}
#| echo: false

scatter = alt.Chart(sacramento).mark_circle().encode(
    x=alt.X("sqft")
        .scale(zero=False)
        .title("House size (square feet)"),
    y=alt.Y("price")
        .axis(format="$,.0f")
        .title("Price (USD)")
)

scatter

```

## K-nearest neighbors regression

```{python}
# look at a small sample of data
np.random.seed(10)

small_sacramento = sacramento.sample(n=30)
print(small_sacramento)
```

### Sample: Example

::: {.columns}
::: {.column}

House price of 2000

```{python}
# | echo: false

small_plot = (
	alt.Chart(small_sacramento)
	.mark_circle(opacity=1)
	.encode(
		x=alt.X('sqft').scale(zero=False).title('House size (square feet)'),
		y=alt.Y('price').axis(format='$,.0f').title('Price (USD)'),
	)
)

# add an overlay to the base plot
line_df = pd.DataFrame({'x': [2000]})
rule = (
	alt.Chart(line_df)
	.mark_rule(strokeDash=[6], size=1.5, color='black')
	.encode(x='x')
)

(small_plot + rule)
```

:::
::: {.column}

5 closest neighbors

```{python}
# | echo: false

small_sacramento['dist'] = (2000 - small_sacramento['sqft']).abs()
nearest_neighbors = small_sacramento.nsmallest(5, 'dist')
nearest_neighbors

nn_plot = small_plot + rule

# plot horizontal lines which is perpendicular to x=2000
h_lines = []
for i in range(5):
	h_line_df = pd.DataFrame(
		{
			'sqft': [nearest_neighbors.iloc[i, 4], 2000],
			'price': [nearest_neighbors.iloc[i, 6]] * 2,
		}
	)
	h_lines.append(
		alt.Chart(h_line_df)
		.mark_line(color='black')
		.encode(x='sqft', y='price')
	)

# highlight the nearest neighbors in orange
orange_neighbrs = (
	alt.Chart(nearest_neighbors)
	.mark_circle(opacity=1, color='#ff7f0e')
	.encode(
		x=alt.X('sqft').scale(zero=False).title('House size (square feet)'),
		y=alt.Y('price').axis(format='$,.0f').title('Price (USD)'),
	)
)

nn_plot = alt.layer(*h_lines, small_plot, orange_neighbrs, rule)

nn_plot
```

:::
:::

### Sample: Predict

5 closest points

```{python}
small_sacramento['dist'] = (2000 - small_sacramento['sqft']).abs()
nearest_neighbors = small_sacramento.nsmallest(5, 'dist')
print(nearest_neighbors)
```

Average of nearest points

```{python}
prediction = nearest_neighbors['price'].mean()
print(prediction)
```

### Sample: Visualize new prediction


::: {.columns}
::: {.column}

```{python}
#| echo: false

nn_plot_pred = nn_plot + alt.Chart(
    pd.DataFrame({"sqft": [2000], "price": [prediction]})
).mark_circle(size=80, opacity=1, color="#d62728").encode(x="sqft", y="price")

nn_plot_pred
```

:::

::: {.column}

```{python}
print(prediction)
```

:::
:::

## Training, evaluating, and tuning the model

```{python}
np.random.seed(1)

sacramento_train, sacramento_test = train_test_split(
    sacramento, train_size=0.75
)
```

:::{.callout-note}
We are not specifying the stratify argument.
The `train_test_split()` function cannot stratify on a quantitative variable
:::

### Metric: RMS(P)E

Root Mean Square (Prediction) Error

$$\text{RMSPE} = \sqrt{\frac{1}{n}\sum\limits_{i=1}^{n}(y_i - \hat{y}_i)^2}$$

where:

- $n$ is the number of observations,
- $y_i$ is the observed value for the $i^\text{th}$ observation, and
- $\hat{y}_i$ is the forecasted/predicted value for the $i^\text{th}$ observation.

### Metric: Visualize

```{python}
# | echo: false


from sklearn.neighbors import KNeighborsRegressor

# (synthetic) new prediction points
pts = pd.DataFrame(
	{'sqft': [1200, 1850, 2250], 'price': [300000, 200000, 500000]}
)
finegrid = pd.DataFrame({'sqft': np.arange(600, 3901, 10)})

# preprocess the data, make the pipeline
sacr_preprocessor = make_column_transformer((StandardScaler(), ['sqft']))
sacr_pipeline = make_pipeline(
	sacr_preprocessor, KNeighborsRegressor(n_neighbors=4)
)

# fit the model
X = small_sacramento[['sqft']]
y = small_sacramento[['price']]
sacr_pipeline.fit(X, y)

# predict on the full grid and new data pts
sacr_full_preds_hid = pd.concat(
	(
		finegrid,
		pd.DataFrame(sacr_pipeline.predict(finegrid), columns=['predicted']),
	),
	axis=1,
)

sacr_new_preds_hid = pd.concat(
	(
		small_sacramento[['sqft', 'price']].reset_index(),
		pd.DataFrame(
			sacr_pipeline.predict(small_sacramento[['sqft', 'price']]),
			columns=['predicted'],
		),
	),
	axis=1,
).drop(columns=['index'])

# to make altair mark_line works, need to create separate dataframes for each vertical error line
errors_plot = (
	small_plot
	+ alt.Chart(sacr_full_preds_hid)
	.mark_line(color='#ff7f0e')
	.encode(x='sqft', y='predicted')
	+ alt.Chart(sacr_new_preds_hid)
	.mark_circle(opacity=1)
	.encode(x='sqft', y='price')
)
sacr_new_preds_melted_df = sacr_new_preds_hid.melt(id_vars=['sqft'])
v_lines = []
for i in sacr_new_preds_hid['sqft']:
	line_df = sacr_new_preds_melted_df.query(f'sqft == {i}')
	v_lines.append(
		alt.Chart(line_df).mark_line(color='black').encode(x='sqft', y='value')
	)

errors_plot = alt.layer(*v_lines, errors_plot)
errors_plot
```

### RMSPE vs RMSE

Root Mean Square (Prediction) Error

- RMSPE: the error calculated on the non-training dataset
- RMSE: the error calcualted on the training dataset

This notation is a statistics distinction,
you will most likely see RMSPE written as RMSE.

### Pick the best k: `GridSearchCV()`

We'll use cross validation to try out many different values of `k`

```{python}
# import the K-NN regression model
from sklearn.neighbors import KNeighborsRegressor

# preprocess the data, make the pipeline
sacr_preprocessor = make_column_transformer((StandardScaler(), ['sqft']))
sacr_pipeline = make_pipeline(sacr_preprocessor, KNeighborsRegressor())

# create the 5-fold GridSearchCV object
param_grid = {
	'kneighborsregressor__n_neighbors': range(1, 201, 3),
}
sacr_gridsearch = GridSearchCV(
	estimator=sacr_pipeline,
	param_grid=param_grid,
	cv=5,
	scoring='neg_root_mean_squared_error',  # we will deal with this later
)

```

### Pick the best k: fit the CV models

```{python}
# fit the GridSearchCV object
sacr_gridsearch.fit(
	sacramento_train[['sqft']],  # A single-column data frame
	sacramento_train['price'],  # A series
)

# Retrieve the CV scores
sacr_results = pd.DataFrame(sacr_gridsearch.cv_results_)
sacr_results['sem_test_score'] = sacr_results['std_test_score'] / 5 ** (1 / 2)
sacr_results = sacr_results[
	[
		'param_kneighborsregressor__n_neighbors',
		'mean_test_score',
		'sem_test_score',
	]
].rename(columns={'param_kneighborsregressor__n_neighbors': 'n_neighbors'})
print(sacr_results)
```

### Look at the CV Results

```{python}
print(sacr_results)
```

- `n_neighbors`: values of $K$
- `mean_test_score`: RMSPE estimated via cross-validation (it's negative!)
- `sem_test_score`: standard error of our cross-validation RMSPE estimate
  (how uncertain we are in the mean value)

```{python}
sacr_results['mean_test_score'] = -sacr_results['mean_test_score']
print(sacr_results)
```

### Best k

take the *minimum* RMSPE to find the best setting for the number of neighbors

```{python}
best_k_sacr = sacr_results["n_neighbors"][sacr_results["mean_test_score"].idxmin()]
best_cv_RMSPE = min(sacr_results["mean_test_score"])
```

Best k:

::: {.columns}
::: {.column}

```{python}
best_k_sacr
```
:::

::: {.column}

```{python}
best_cv_RMSPE
```

:::
:::

### Best k: Visualize

```{python}
# | echo: false

sacr_tunek_plot = (
	alt.Chart(sacr_results)
	.mark_line(point=True)
	.encode(
		x=alt.X('n_neighbors:Q', title='Neighbors'),
		y=alt.Y(
			'mean_test_score',
			scale=alt.Scale(zero=False),
			title='Cross-Validation RMSPE Estimate',
		),
	)
)

sacr_tunek_plot

```

```{python}
sacr_gridsearch.best_params_

```

## Underfitting and overfitting

::: {.columns}
::: {.column}

```{python}
sacr_tunek_plot
```
:::

::: {.column}
The RMSPE values start to get higher after a certain k value
:::
:::

### Visualizing different values of k

```{python}
# | echo: false

gridvals = [
	1,
	3,
	25,
	best_k_sacr,
	250,
	len(sacramento_train),
]

plots = list()

sacr_preprocessor = make_column_transformer((StandardScaler(), ['sqft']))
X = sacramento_train[['sqft']]
y = sacramento_train[['price']]

base_plot = (
	alt.Chart(sacramento_train)
	.mark_circle()
	.encode(
		x=alt.X(
			'sqft',
			title='House size (square feet)',
			scale=alt.Scale(zero=False),
		),
		y=alt.Y('price', title='Price (USD)', axis=alt.Axis(format='$,.0f')),
	)
)
for i in range(len(gridvals)):
	# make the pipeline based on n_neighbors
	sacr_pipeline = make_pipeline(
		sacr_preprocessor, KNeighborsRegressor(n_neighbors=gridvals[i])
	)
	sacr_pipeline.fit(X, y)
	# predictions
	sacr_preds = sacramento_train
	sacr_preds = sacr_preds.assign(
		predicted=sacr_pipeline.predict(sacramento_train)
	)
	# overlay the plots
	plots.append(
		base_plot
		+ alt.Chart(sacr_preds, title=f'K = {gridvals[i]}')
		.mark_line(color='#ff7f0e')
		.encode(x='sqft', y='predicted')
	)

(plots[0] | plots[1] | plots[2]) & (plots[3] | plots[4] | plots[5])
```

## Evaluating on the test set

RMSPE on the test data

- Retrain the K-NN regression model on the entire training data set using best k

```{python}
from sklearn.metrics import mean_squared_error

sacramento_test['predicted'] = sacr_gridsearch.predict(sacramento_test)
RMSPE = mean_squared_error(
	y_true=sacramento_test['price'], y_pred=sacramento_test['predicted']
) ** (1 / 2)

RMSPE
```

### Final best K model

Predicted values of house price (orange line) for the final K-NN regression model.

```{python}
# | echo: false

# Create a grid of evenly spaced values along the range of the sqft data
sqft_prediction_grid = pd.DataFrame(
	{'sqft': np.arange(sacramento['sqft'].min(), sacramento['sqft'].max(), 10)}
)
# Predict the price for each of the sqft values in the grid
sqft_prediction_grid['predicted'] = sacr_gridsearch.predict(
	sqft_prediction_grid
)

# Plot all the houses
base_plot = (
	alt.Chart(sacramento)
	.mark_circle(opacity=0.4)
	.encode(
		x=alt.X('sqft').scale(zero=False).title('House size (square feet)'),
		y=alt.Y('price').axis(format='$,.0f').title('Price (USD)'),
	)
)

# Add the predictions as a line
sacr_preds_plot = base_plot + alt.Chart(
	sqft_prediction_grid, title=f'K = {best_k_sacr}'
).mark_line(color='#ff7f0e').encode(x='sqft', y='predicted')

sacr_preds_plot
```

## Multivariable K-NN regression

We can use multiple predictors in K-NN regression

### Multivariable K-NN regression: Preprocessor

```{python}
sacr_preprocessor = make_column_transformer(
	(StandardScaler(), ['sqft', 'beds'])
)
sacr_pipeline = make_pipeline(sacr_preprocessor, KNeighborsRegressor())
```

### Multivariable K-NN regression: CV

```{python}
# create the 5-fold GridSearchCV object
param_grid = {
	'kneighborsregressor__n_neighbors': range(1, 50),
}

sacr_gridsearch = GridSearchCV(
	estimator=sacr_pipeline,
	param_grid=param_grid,
	cv=5,
	scoring='neg_root_mean_squared_error',
)

sacr_gridsearch.fit(
	sacramento_train[['sqft', 'beds']], sacramento_train['price']
)
```

### Multivariable K-NN regression: Best K

```{python}
# retrieve the CV scores
sacr_results = pd.DataFrame(sacr_gridsearch.cv_results_)
sacr_results['sem_test_score'] = sacr_results['std_test_score'] / 5 ** (1 / 2)
sacr_results['mean_test_score'] = -sacr_results['mean_test_score']
sacr_results = sacr_results[
	[
		'param_kneighborsregressor__n_neighbors',
		'mean_test_score',
		'sem_test_score',
	]
].rename(columns={'param_kneighborsregressor__n_neighbors': 'n_neighbors'})

# show only the row of minimum RMSPE
sacr_results.nsmallest(1, 'mean_test_score')
```

### Multivariable K-NN regression: Best model

```{python}
best_k_sacr_multi = sacr_results["n_neighbors"][sacr_results["mean_test_score"].idxmin()]
min_rmspe_sacr_multi = min(sacr_results["mean_test_score"])
```

Best K

```{python}
best_k_sacr_multi
```

Best RMSPE

```{python}
min_rmspe_sacr_multi
```

### Multivariable K-NN regression: Test data

```{python}
sacramento_test["predicted"] = sacr_gridsearch.predict(sacramento_test)
RMSPE_mult = mean_squared_error(
    y_true=sacramento_test["price"],
    y_pred=sacramento_test["predicted"]
)**(1/2)

RMSPE_mult
```

### Multivariable K-NN regression: Visualize

```{python}
# | echo: false

# create a prediction pt grid
xvals = np.linspace(
	sacramento_train['sqft'].min(), sacramento_train['sqft'].max(), 50
)
yvals = np.linspace(
	sacramento_train['beds'].min(), sacramento_train['beds'].max(), 50
)
xygrid = np.array(np.meshgrid(xvals, yvals)).reshape(2, -1).T
xygrid = pd.DataFrame(xygrid, columns=['sqft', 'beds'])

# add prediction
knnPredGrid = sacr_gridsearch.predict(xygrid)

fig = px.scatter_3d(
	sacramento_train,
	x='sqft',
	y='beds',
	z='price',
	opacity=0.4,
	labels={
		'sqft': 'Size (sq ft)',
		'beds': 'Bedrooms',
		'price': 'Price (USD)',
	},
)

fig.update_traces(marker={'size': 2, 'color': 'red'})

fig.add_trace(
	go.Surface(
		x=xvals,
		y=yvals,
		z=knnPredGrid.reshape(50, -1),
		name='Predictions',
		colorscale='viridis',
		colorbar={'title': 'Price (USD)'},
	)
)

fig.update_layout(
	margin=dict(l=0, r=0, b=0, t=1),
	template='plotly_white',
)

fig
```

## Strengths and limitations of K-NN regression


Strengths:

- simple, intuitive algorithm
- requires few assumptions about what the data must look like
- works well with non-linear relationships (i.e., if the relationship is not a straight line)

Weaknesses:

- very slow as the training data gets larger
- may not perform well with a large number of predictors
- may not predict well beyond the range of values input in your training data

## Linear Regression



## References

Thomas Cover and Peter Hart. Nearest neighbor pattern classification. IEEE Transactions on Information Theory, 13(1):21–27, 1967.

Evelyn Fix and Joseph Hodges. Discriminatory analysis. nonparametric discrimination: consistency properties. Technical Report, USAF School of Aviation Medicine, Randolph Field, Texas, 1951.
