---
title: "Tree-based and ensemble models"
format: 
  revealjs:
    slide-number: true
    slide-level: 4
    smaller: true
    theme: simple
jupyter: python3
execute:
  echo: true
  warning: false
---

```{python}
#| include: false
import pandas as pd
pd.set_option('display.max_rows', 5)
```

## Tree-based methods

- Algorithms that stratifying or segmenting the predictor space
  into a number of simple regions.

- We call these algorithms decision-tree methods 
 because the decisions used to segment the predictor space 
 can be summarized in a tree.
 
- Decision trees on their own, are very explainable and intuitive,
  but not very powerful at predicting. 
  
- However, there are extensions of decision trees, 
  such as random forest and boosted trees,
  which are very powerful at predicting. 
  We will demonstrate two of these in this session.

## Decision trees

- [Decision Trees](https://mlu-explain.github.io/decision-tree/)  
  by Jared Wilber & Lucía Santamaría

## Example: the heart data set

:::: {.columns}
::: {.column width="50%"}
- Let's consider a situation where we'd like to be able to predict 
  the presence of heart disease (`AHD`) in patients, 
  based off 13 measured characteristics.

- The [heart data set](https://www.statlearning.com/s/Heart.csv) 
  contains a binary outcome for heart disease 
  for patients who presented with chest pain.
:::

::: {.column width="50%"}
```{python}
heart = pd.read_csv("data/Heart.csv", index_col=0)
heart.info()
```
:::
::::

## Example: the heart data set

- An angiographic test was performed and a label for `AHD` of Yes
  was labelled to indicate the presence of heart disease,
  otherwise the label was No.
  
```{python}
heart.head()
```

## Do we have a class imbalance?

It's always important to check this, as it may impact your splitting 
and/or modeling decisions.

```{python}
heart['AHD'].value_counts(normalize=True)
```

This looks pretty good! 
We can move forward this time without doing much more about this.

## Categorical variables

:::: {.columns}
::: {.column width="35%"}
- This is our first case of seeing categorical predictor variables, 
can we treat them the same as numerical ones? **No!** 

- In `scikit-learn` we must perform **one-hot encoding**
:::

::: {.column width="65%"}
![](https://scales.arabpsychology.com/wp-content/uploads/2024/05/onehot1-1-1.png)

*Source: <https://scales.arabpsychology.com/stats/how-can-i-perform-one-hot-encoding-in-r/>*
:::
::::

## Data splitting

Let's split the data into training and test sets:

```{python}
from sklearn.model_selection import train_test_split

heart_train, heart_test = train_test_split(
    heart, train_size=0.75, stratify=heart["AHD"]
)

X_train = heart_train.drop(columns=['AHD'])
y_train = heart_train['AHD']
X_test = heart_test.drop(columns=['AHD'])
y_test = heart_test['AHD']
```

## One hot encoding & pre-processing

```{python}
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import make_column_transformer, make_column_selector

numeric_feats = ['Age', 'RestBP', 'Chol', 'RestECG', 'MaxHR', 'Oldpeak','Slope', 'Ca']
passthrough_feats = ['Sex', 'Fbs', 'ExAng']
categorical_feats = ['ChestPain', 'Thal']

heart_preprocessor = make_column_transformer(
    (StandardScaler(), numeric_feats), 
    ("passthrough", passthrough_feats),     
    (OneHotEncoder(handle_unknown = "ignore"), categorical_feats),     
)
```


## Fitting a dummy classifier

```{python}
from sklearn.dummy import DummyClassifier
from sklearn.pipeline import make_pipeline
from sklearn.model_selection import cross_validate

dummy = DummyClassifier()
dummy_pipeline = make_pipeline(heart_preprocessor, dummy)
cv_10_dummy = pd.DataFrame(
    cross_validate(
        estimator=dummy_pipeline,
        cv=10,
        X=X_train,
        y=y_train
    )
)
cv_10_dummy_metrics = cv_10_dummy.agg(["mean", "sem"])
results = pd.DataFrame({'mean' : [cv_10_dummy_metrics.test_score.iloc[0]],
  'sem' : [cv_10_dummy_metrics.test_score.iloc[1]]},
  index = ['Dummy classifier']
)
results
```


## Fitting a decision tree

```{python}
from sklearn.tree import DecisionTreeClassifier
from sklearn.pipeline import make_pipeline

decision_tree = DecisionTreeClassifier()

dt_pipeline = make_pipeline(heart_preprocessor, decision_tree)
cv_10_dt = pd.DataFrame(
    cross_validate(
        estimator=dt_pipeline,
        cv=10,
        X=X_train,
        y=y_train
    )
)
cv_10_dt_metrics = cv_10_dt.agg(["mean", "sem"])
results_dt = pd.DataFrame({'mean' : [cv_10_dt_metrics.test_score.iloc[0]],
  'sem' : [cv_10_dt_metrics.test_score.iloc[1]]},
  index = ['Decision tree']
)
results = pd.concat([results, results_dt])
results
```

## Can we do better?

- We could tune some decision tree parameters 
  (e.g., Gini impurity, maximum tree depth, etc)...

- We could also try a different tree-based method!

- [The Random Forest Algorithm](https://mlu-explain.github.io/random-forest/) 
  by Jenny Yeon & Jared Wilber
  
## Random forest in `scikit-learn` & missing values

:::: {.columns}
::: {.column width="35%"}
- Does not accept missing values, we need to deal with these somehow...

- We can either drop the observations with missing values, 
  or we can somehow impute them.

- For the purposes of this demo we will drop them, 
  but if you are interested in imputation, 
  see the imputation tutorial in 
  [`scikit-learn`](https://scikit-learn.org/stable/modules/impute.html)
:::

::: {.column width="50%"}
How many rows have missing observations:

```{python}
heart.isna().any(axis=1).sum()
```

Drop rows with missing observations:

```{python}
heart_train_drop_na = heart_train.dropna()

X_train_drop_na = heart_train_drop_na.drop(columns=['AHD'])
y_train_drop_na = heart_train_drop_na['AHD']
```
:::
::::

## Random forest in `scikit-learn`

```{python}
from sklearn.ensemble import RandomForestClassifier

random_forest = RandomForestClassifier()

rf_pipeline = make_pipeline(heart_preprocessor, random_forest)
cv_10_rf = pd.DataFrame(
    cross_validate(
        estimator=rf_pipeline,
        cv=10,
        X=X_train_drop_na,
        y=y_train_drop_na
    )
)

cv_10_rf_metrics = cv_10_rf.agg(["mean", "sem"])
results_rf = pd.DataFrame({'mean' : [cv_10_rf_metrics.test_score.iloc[0]],
  'sem' : [cv_10_rf_metrics.test_score.iloc[1]]},
  index = ['Random forest']
)
results = pd.concat([results, results_rf])
results
```

## Can we do better?

- Random forest can be tuned a several important parameters, including:

  - `n_estimators`: number of decision trees (higher = more complexity)

  - `max_depth`: max depth of each decision tree (higher = more complexity)

  - `max_features`: the number of features you get to look at each split 
  (higher = more complexity)
  
- We can use `GridSearchCV` to search for the optimal parameters for these,
  as we did for $K$ in $K$-nearest neighbors.

## Tuning random forest in `scikit-learn`

```{python}
from sklearn.model_selection import GridSearchCV

random_forest_tuned = RandomForestClassifier()
rf_tuned_pipeline = make_pipeline(heart_preprocessor, random_forest_tuned)

rf_param_grid = {'randomforestclassifier__n_estimators': [100, 150, 200, 250],
              'randomforestclassifier__max_depth': [1, 3, 5, 7, 9],
              'randomforestclassifier__max_features': [1, 2, 3, 4, 5, 6, 7]}

rf_tune_grid = GridSearchCV(
    estimator=rf_tuned_pipeline,
    param_grid=rf_param_grid,
    cv=10,
    n_jobs=-1 # tells computer to use all available CPUs
)

rf_tune_grid.fit(
    X_train_drop_na,
    y_train_drop_na
)

accuracies_grid = pd.DataFrame(rf_tune_grid.cv_results_)
accuracies_grid.info()
```

```{python}
accuracies_grid = pd.DataFrame(rf_tune_grid.cv_results_)
#accuracies_grid.info()
accuracies_grid[['param_randomforestclassifier__max_depth', 
'param_randomforestclassifier__max_features',
'param_randomforestclassifier__n_estimators',
'mean_test_score',
'std_test_score']]
#accuracies_grid[['param_randomforestclassifier__max_depth', 'mean_test_score']]
rf_tune_grid.best_params_
```

## Boosting

- TBD

## Boosting with `lightgbm`